# üìò Daily Learning Log ‚Äî 2025-10-30

### Chapter/Section
> Chapter 1: An Introduction to Large Language Models (Pages 25-36)

---

## üßæ Raw Extract
*(PDF pages 25-36 - automatically extracted)*

---

## üß† Copilot Notes (Generated)

1) TL;DR (90 seconds)
- Core idea: Language AI evolved from simple word counts (bag-of-words) to meaning-aware vector embeddings and sequence models with attention‚Äîpaving the way to today‚Äôs LLMs.
- Why it matters: To build practical RAG, agents, and pipelines (AutoGen/LangChain), you need to turn text into tokens and vectors, understand how models generate text autoregressively, and why attention beats older RNNs.
- Key takeaway: Text must be tokenized, embedded (vectors), and then modeled with sequence architectures; attention lets models focus on the right parts, enabling quality and speed.
- Connection to future topics: These basics lead directly to Transformers, RAG/vector stores, prompt/token budgeting, and fine-tuning.
- Common pitfall: Assuming word counts = meaning, or ignoring tokenization details (which affect cost, context length, and retrieval quality).

2) Background Concepts (Prerequisites)
TOKENIZATION
- What: Split text into small units (tokens) that a model understands.
- Why: All LLM I/O is tokenized; affects cost, context fit, and performance.
- Code (Colab-ready):
  ```python
  !pip -q install transformers
  from transformers import AutoTokenizer

  text = "I love llamas!"
  tok = AutoTokenizer.from_pretrained("distilbert-base-uncased")
  ids = tok.encode(text, add_special_tokens=True)
  print("Tokens:", tok.convert_ids_to_tokens(ids))
  print("IDs:", ids)
  ```
- Bridge: In LangChain/AutoGen, chunking, prompt size, and tool outputs depend on tokenization; plan retrieval chunk sizes using tokenizer.

EMBEDDING
- What: Turn text (word/sentence) into a numeric vector that captures meaning.
- Why: Power semantic search, RAG, clustering, deduping, intent classification.
- Code:
  ```python
  !pip -q install sentence-transformers
  from sentence_transformers import SentenceTransformer, util

  model = SentenceTransformer("all-MiniLM-L6-v2")  # small, fast
  sents = ["I deposited money at the bank.", "The river bank was steep.",
           "I went to the financial institution."]
  embs = model.encode(sents, normalize_embeddings=True)
  sim = lambda i,j: float(util.cos_sim(embs[i], embs[j]))
  print("bank(financial) vs bank(river):", sim(0,1))
  print("bank(financial) vs financial institution:", sim(0,2))
  ```
- Bridge: In LangChain, these embeddings go into vector stores (FAISS, Chroma) for RAG; in AutoGen, agents retrieve grounded info using embeddings.

AUTOREGRESSIVE
- What: Predict the next token using previous tokens.
- Why: Core of how GPT-like models generate text; informs sampling, temperature.
- Code:
  ```python
  from transformers import AutoModelForCausalLM, AutoTokenizer
  import torch
  model_id = "sshleifer/tiny-gpt2"  # tiny, demo-only
  tok = AutoTokenizer.from_pretrained(model_id)
  model = AutoModelForCausalLM.from_pretrained(model_id)
  prompt = "A llama walks into a library and"
  inputs = tok(prompt, return_tensors="pt")
  out = model.generate(**inputs, max_new_tokens=20, do_sample=True, temperature=0.9)
  print(tok.decode(out[0], skip_special_tokens=True))
  ```
- Bridge: In agents/pipelines, every call is autoregressive generation; parameters like temperature/top_p change behavior.

RNN
- What: Recurrent Neural Network‚Äîprocesses sequences step-by-step with a hidden state.
- Why: Pre-Transformer sequence modeling; helps explain encoder-decoder translation and why Transformers improved on it.
- Code:
  ```python
  import torch, torch.nn as nn
  torch.manual_seed(0)
  rnn = nn.RNN(input_size=4, hidden_size=8, batch_first=True)
  x = torch.randn(1, 5, 4)   # batch=1, seq=5, feat=4
  h0 = torch.zeros(1, 1, 8)  # num_layers=1, batch=1, hidden=8
  out, hN = rnn(x, h0)
  print("Seq outputs:", out.shape, "Final hidden:", hN.shape)
  ```
- Bridge: RNNs explain encoder-decoder NMT and teacher forcing; Transformers dropped recurrence for parallelism.

ATTENTION
- What: Mechanism to focus on the most relevant parts of the input when producing each output.
- Why: Enables contextual understanding (which ‚Äúbank‚Äù?), long-range dependencies, and parallel training in Transformers.
- Code (toy scaled dot-product attention):
  ```python
  import torch, torch.nn.functional as F
  torch.manual_seed(0)
  Q = torch.randn(1, 1, 4)   # query (e.g., current output step)
  K = torch.randn(1, 5, 4)   # keys (encoder states)
  V = torch.randn(1, 5, 6)   # values (encoder states to mix)
  attn_scores = (Q @ K.transpose(-2, -1)) / (4 ** 0.5)  # [1,1,5]
  attn_weights = F.softmax(attn_scores, dim=-1)          # sum to 1
  context = attn_weights @ V                              # [1,1,6]
  print("Weights:", attn_weights.squeeze().tolist())
  print("Context shape:", context.shape)
  ```
- Bridge: In LangChain/AutoGen, you don‚Äôt code attention directly, but it‚Äôs why Transformers are fast/accurate and scale to big prompts.

3) Main Content (Code-First!)
Topic: What Is Language AI?
Quick Code Example:
```python
from transformers import pipeline
gen = pipeline("text-generation", model="sshleifer/tiny-gpt2")
print(gen("Translate to Spanish: I love llamas.")[0]["generated_text"])
```
What‚Äôs Happening:
- We load a tiny language model pipeline.
- It autoregressively generates text, showing ‚Äúlanguage tasks‚Äù like translation-ish behavior (tiny model is weak but illustrative).
Theory:
- Language AI covers systems that understand/generate human language: translation, summarization, Q&A, retrieval.
- LLMs are current state-of-the-art for many tasks, but smaller/alternative models remain useful.
Connection to AutoGen/Modern Tools:
- Agents and tool-use sit atop LLMs; retrieval systems (RAG) give LLMs up-to-date knowledge.
Common Mistakes:
- Treating LLMs as databases. Use RAG or tool calling for factuality and freshness.

Topic: Representing Language as a Bag-of-Words
Quick Code Example:
```python
!pip -q install scikit-learn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

docs = [
    "I love llamas",
    "Llamas love me",
    "I deposit money at the bank",
    "The river bank is steep"
]
cv = CountVectorizer(lowercase=True, stop_words=None)
X = cv.fit_transform(docs)
print("Vocab:", cv.get_feature_names_out())
print("BoW vectors shape:", X.shape)
print("Cosine(sim bank financial vs bank river):",
      float(cosine_similarity(X[2], X[3])))
```
What‚Äôs Happening:
- CountVectorizer tokenizes (whitespace) and counts word frequencies.
- We compute cosine similarity between documents.
Expected Output:
- Vocabulary list, 4x|vocab| sparse matrix, and a numeric similarity (>0 if words overlap).
Theory:
- Bag-of-words ignores word order and meaning; works surprisingly well for some tasks (classic text classification) but can‚Äôt disambiguate ‚Äúbank.‚Äù
Connection to AutoGen/Modern Tools:
- Still useful for quick keyword filters or hybrid retrieval (BM25 + embeddings) to improve recall before re-ranking.
Common Mistakes:
- Assuming counts capture semantics; ignoring tokenization effects (stopwords, case).

Topic: Better Representations with Dense Vector Embeddings (word2vec ‚Üí modern embeddings)
Quick Code Example:
```python
!pip -q install gensim
import gensim.downloader as api

wv = api.load("glove-wiki-gigaword-50")  # ~66MB
for w in ["king", "queen", "bank", "river"]:
    print("Nearest to", w, ":", wv.most_similar(w)[:3])
print("Similarity(bank, river):", wv.similarity("bank", "river"))
print("Similarity(bank, money):", wv.similarity("bank", "money"))
```
What‚Äôs Happening:
- Load small pre-trained word vectors.
- Inspect nearest neighbors and similarities; see ‚Äúbank‚Äù closer to ‚Äúmoney‚Äù than ‚Äúriver‚Äù in many corpora (polysemy limitation of static embeddings).
Theory:
- word2vec (2013) learns word embeddings by predicting neighbors; embeddings capture distributional meaning (‚Äúyou know a word by the company it keeps‚Äù).
- Static embeddings give each word one vector‚Äîcontext insensitive (bank financial vs river).
Connection to AutoGen/Modern Tools:
- Modern sentence/ctx embeddings (e.g., all-MiniLM) power vector databases for RAG; far better than BoW for meaning and multilinguality.
Common Mistakes:
- Mixing static word vectors and sentence similarity; prefer sentence embeddings for retrieval.

Topic: Encoding and Decoding Context with RNNs (and the rise of Attention)
Quick Code Example (toy attention over encoder states):
```python
import torch, torch.nn.functional as F
torch.manual_seed(0)
# Pretend these are encoder hidden states for "I love llamas"
encoder_states = torch.randn(1, 3, 8)  # [batch, seq_len, hidden]
# Decoder query while generating next word (e.g., Dutch "lama‚Äôs")
query = torch.randn(1, 1, 8)

attn_raw = query @ encoder_states.transpose(-2, -1) / (8 ** 0.5)  # [1,1,3]
attn_w = F.softmax(attn_raw, dim=-1)
context = attn_w @ encoder_states  # [1,1,8]
print("Attention weights over [I, love, llamas]:", attn_w.squeeze().tolist())
```
What‚Äôs Happening:
- Simulate how a decoder focuses on specific source words (here, likely ‚Äúllamas‚Äù) when generating the next token in translation.
- We compute attention weights and produce a context vector.
Expected Output:
- A 3-element distribution summing to 1; the largest weight corresponds to the most relevant source token.
Theory:
- Encoder-decoder RNNs read a source sentence into hidden states, then decode target tokens autoregressively.
- Without attention, long sentences compress into a single vector‚Äîinformation bottleneck.
- Attention (2014) lets the decoder ‚Äúlook back‚Äù at all encoder states per output step, improving quality and handling longer sequences.
- RNNs are sequential (no parallelization), which limits training speed and context length‚ÄîTransformers solved this with self-attention everywhere.
Connection to AutoGen/Modern Tools:
- Today‚Äôs LLMs use Transformers (self-attention) rather than RNNs; attention maps underlie models‚Äô ability to handle long prompts and retrieval chunks.
Common Mistakes:
- Thinking attention = explainability; it‚Äôs a mechanism, not a faithful explanation of reasoning.
- Expecting static embeddings to handle polysemy; contextual models (Transformers) do better.

Topic: Autoregression in Practice
Quick Code Example:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_id = "sshleifer/tiny-gpt2"
tok = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
prompt = "User: Summarize: Llamas are social animals.\nAssistant:"
inputs = tok(prompt, return_tensors="pt")
out = model.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=0.8, top_p=0.9)
print(tok.decode(out[0], skip_special_tokens=True))
```
What‚Äôs Happening:
- The model consumes all previous tokens to predict the next one‚Äîthen repeats (greedy/sampling).
- temperature/top_p control creativity vs determinism.
Theory:
- p(next_token | previous_tokens) chaining defines generation.
- This is why prompt engineering matters: you set the initial probability mass landscape.
Connection to AutoGen/Modern Tools:
- Agents and chains rely on controlling generation parameters and structuring prompts/tool outputs for predictable behavior.
Common Mistakes:
- Using high temperature for tasks that require exactness (e.g., parsing); prefer low temperature or constrained decoding.

4) Hands-On Challenge (10 min)
Goal: Feel the difference between BoW and embeddings, and see tokenization effects.
Task:
- Compare similarities among three sentences using BoW vs sentence embeddings:
  - A: ‚ÄúI deposited money at the bank.‚Äù
  - B: ‚ÄúThe river bank was steep.‚Äù
  - C: ‚ÄúI went to the financial institution.‚Äù
- Then change tokenization (remove stopwords vs keep; lowercase vs not) and observe BoW changes.

Starter Code:
```python
!pip -q install scikit-learn sentence-transformers
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util

A = "I deposited money at the bank."
B = "The river bank was steep."
C = "I went to the financial institution."
docs = [A, B, C]

# BoW 1: default
cv1 = CountVectorizer()
X1 = cv1.fit_transform(docs)
print("BoW(default) sim A-C:", float(cosine_similarity(X1[0], X1[2])))

# BoW 2: lowercase, remove stopwords
cv2 = CountVectorizer(lowercase=True, stop_words="english")
X2 = cv2.fit_transform(docs)
print("BoW(stopwords removed) sim A-C:", float(cosine_similarity(X2[0], X2[2])))

# Sentence embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")
E = model.encode(docs, normalize_embeddings=True)
def cs(i,j): return float(util.cos_sim(E[i], E[j]))
print("Embeddings sim A-C:", cs(0,2))
print("Embeddings sim A-B:", cs(0,1))
```
Expected learning:
- BoW gives low A‚ÄìC similarity (no shared words) even though they mean similar things.
- Embeddings boost A‚ÄìC similarity because they capture semantics.
- Tokenization choices (stopwords/lowercasing) shift BoW results‚Äîimportant for retrieval and cost.

5) Concept Mastery Check (new concepts covered today)
- Language AI vs LLMs (scope and use cases)
- Tokenization (and why it matters)
- Bag-of-Words (count-based representation)
- Embeddings (word vs sentence; semantic similarity)
- word2vec (distributional training idea; static embeddings)
- RNN encoder-decoder (sequence-to-sequence)
- Autoregressive generation (next-token prediction)
- Attention mechanism (focus over inputs; context handling)

Bonus bridges to your tooling:
- LangChain/AutoGen: Use tokenizer-aware chunking; store sentence embeddings in a vector DB; hybrid search (BM25 + embeddings); set generation params per task; prefer contextual embeddings for RAG over static word vectors.

## üì∏ Extracted Images

### Figure 1 - Page 27

![Figure 1](temp_images/page_27_img_0.png)

*Image 1 from page 27 (1439x615px)*

---

### Figure 2 - Page 28

![Figure 2](temp_images/page_28_img_0.png)

*Image 2 from page 28 (1022x738px)*

---

### Figure 3 - Page 28

![Figure 3](temp_images/page_28_img_1.png)

*Image 3 from page 28 (923x311px)*

---

### Figure 4 - Page 29

![Figure 4](temp_images/page_29_img_0.png)

*Image 4 from page 29 (934x461px)*

---

### Figure 5 - Page 29

![Figure 5](temp_images/page_29_img_1.png)

*Image 5 from page 29 (1093x530px)*

---

### Figure 6 - Page 30

![Figure 6](temp_images/page_30_img_0.png)

*Image 6 from page 30 (1007x610px)*

---

### Figure 7 - Page 31

![Figure 7](temp_images/page_31_img_0.png)

*Image 7 from page 31 (1233x244px)*

---

### Figure 8 - Page 31

![Figure 8](temp_images/page_31_img_1.png)

*Image 8 from page 31 (1234x509px)*

---

### Figure 9 - Page 32

![Figure 9](temp_images/page_32_img_0.png)

*Image 9 from page 32 (1032x591px)*

---

### Figure 10 - Page 33

![Figure 10](temp_images/page_33_img_0.png)

*Image 10 from page 33 (1437x1002px)*

---

### Figure 11 - Page 34

![Figure 11](temp_images/page_34_img_0.png)

*Image 11 from page 34 (982x674px)*

---

### Figure 12 - Page 34

![Figure 12](temp_images/page_34_img_1.png)

*Image 12 from page 34 (935x483px)*

---

### Figure 13 - Page 35

![Figure 13](temp_images/page_35_img_0.png)

*Image 13 from page 35 (979x818px)*

---

### Figure 14 - Page 36

![Figure 14](temp_images/page_36_img_0.png)

*Image 14 from page 36 (1198x541px)*

---

### Figure 15 - Page 36

![Figure 15](temp_images/page_36_img_1.png)

*Image 15 from page 36 (1217x689px)*

---



---

## üí≠ Reflection (Your Input)
- What clicked for me today:  
- What I still find unclear:  
- How I could apply this in my projects:  
- One analogy or personal insight:

---

## ‚öôÔ∏è Implementation / Experiments
*(Optional ‚Äî record any notebook you ran or code idea you tested)*

```python
# Example placeholder
```

---

## ‚úÖ Summary Sentence

"One line summary of what I learned today."

---

*Generated automatically on 2025-10-30 21:14:18*
